Perfect 🚀 Let’s start with **Module 0 – Setup & Tiny Warmups**.
The goal of this module is to get your environment ready and make sure you can run a trivial agent in a Gym environment before diving into theory.

---

# 🎯 Learning Objectives (Module 0)

1. Install Python + libraries used in RL.
2. Understand the Gymnasium (OpenAI Gym successor) environment interface.
3. Run your **first RL agent** (random policy) on CartPole.
4. Learn how to log episode rewards.

---

## Step 1 — Environment Setup

Make sure you have **Python 3.9+** installed. I recommend using **conda** or **venv**.

```bash
# Create environment
conda create -n rl-basics python=3.10 -y
conda activate rl-basics

# Install core libraries
pip install gymnasium[classic-control] numpy matplotlib

# For deep RL later
pip install torch stable-baselines3[extra] wandb
```

Check install:

```bash
python -c "import gymnasium as gym; print(gym.__version__)"
```

---

## Step 2 — Gymnasium basics

All Gym-like environments follow this API:

```python
import gymnasium as gym

env = gym.make("CartPole-v1", render_mode="human")  # environment
obs, info = env.reset()  # reset, returns initial state

for _ in range(1000):
    action = env.action_space.sample()  # random action
    obs, reward, terminated, truncated, info = env.step(action)
    if terminated or truncated:
        obs, info = env.reset()

env.close()
```

* `obs`: the observation (state vector, here 4 floats).
* `reward`: scalar feedback from env.
* `terminated`: True if episode ended successfully/failure.
* `truncated`: True if episode stopped due to time limit.
* `action_space` & `observation_space`: define possible actions/states.

---

## Step 3 — Your first agent (random policy)

Let’s write a **RandomAgent** class to keep things clean.

```python
import gymnasium as gym
import numpy as np

class RandomAgent:
    def __init__(self, action_space):
        self.action_space = action_space
    
    def act(self, obs):
        return self.action_space.sample()  # pick random action

def run_random_agent(episodes=5):
    env = gym.make("CartPole-v1")
    agent = RandomAgent(env.action_space)

    for ep in range(episodes):
        obs, info = env.reset()
        total_reward = 0
        done = False
        while not done:
            action = agent.act(obs)
            obs, reward, terminated, truncated, info = env.step(action)
            total_reward += reward
            done = terminated or truncated
        print(f"Episode {ep+1} reward: {total_reward}")
    
    env.close()

if __name__ == "__main__":
    run_random_agent()
```

👉 Expected output: You’ll see episode rewards, typically between **10–30** for random actions.

---

## Step 4 — Plotting episode rewards

Let’s log rewards across many episodes.

```python
import matplotlib.pyplot as plt

def run_and_log(episodes=100):
    env = gym.make("CartPole-v1")
    rewards = []

    for ep in range(episodes):
        obs, info = env.reset()
        total_reward = 0
        done = False
        while not done:
            action = env.action_space.sample()
            obs, reward, terminated, truncated, info = env.step(action)
            total_reward += reward
            done = terminated or truncated
        rewards.append(total_reward)
    
    env.close()
    return rewards

rewards = run_and_log(200)
plt.plot(rewards)
plt.xlabel("Episode")
plt.ylabel("Total reward")
plt.title("Random Agent on CartPole")
plt.show()
```

👉 You’ll see noisy rewards, almost always **< 50**.

---

## ✅ Your milestone for Module 0

* [ ] Installed libraries and confirmed Gym works.
* [ ] Ran a random agent in CartPole.
* [ ] Logged rewards over multiple episodes.
* [ ] Understood the **step/reset API** of environments.

---



# structure 2

Absolutely! Let’s do a **deep dive into Module 0: Setup & Tools** — the foundation of your Reinforcement Learning journey.

This module might seem simple, but it's **critical**. A solid setup means you can focus on learning RL, not fighting with installations or environments.

We’ll cover:
- ✅ Python & key libraries
- ✅ Installing and verifying tools
- ✅ Understanding `gymnasium` (the modern Gym)
- ✅ Running and visualizing environments
- ✅ Debugging common issues
- ✅ Writing your first RL loop

Let’s go!

---

## 🔧 Module 0: Deep Dive — Setup & Tools

### 🎯 Goal:
Get your environment ready and run your first RL agent.

---

### 1. ✅ Prerequisites

Make sure you have:
- **Python 3.8+** (recommended: 3.9–3.11)
- **pip** (Python package manager)
- Optional: **conda** or **venv** for environment isolation

> 💡 Tip: Use a virtual environment to avoid conflicts.

#### Create a Virtual Environment (Recommended)

```bash
# Create a virtual environment
python -m venv rl_env

# Activate it
# On Windows:
rl_env\Scripts\activate
# On macOS/Linux:
source rl_env/bin/activate

# Upgrade pip
pip install --upgrade pip
```

---

### 2. 📦 Install Required Libraries

Run this command:

```bash
pip install gymnasium numpy matplotlib pygame torch
```

#### What each library does:
| Library | Purpose |
|--------|--------|
| `gymnasium` | RL environments (replaces old `gym`) |
| `numpy` | Numerical computing (arrays, math) |
| `matplotlib` | Plotting rewards, values, etc. |
| `pygame` | Required for rendering (e.g., visualizing CartPole) |
| `torch` | PyTorch (for deep RL later) |

> ✅ Confirm installation:
```bash
pip list | grep -i "gym\|numpy\|torch"
```

---

### 3. 🧪 Test Your Installation

Let’s write a minimal script to test everything.

#### File: `test_env.py`

```python
import gymnasium as gym
import numpy as np
import time

# Create a simple environment
env = gym.make("CartPole-v1", render_mode="human")

# Reset the environment
state, info = env.reset()
print("Initial state:", state)
print("Info:", info)
print("Action space:", env.action_space)
print("Observation space:", env.observation_space)

# Run a random agent for 100 steps
for step in range(100):
    action = env.action_space.sample()  # Random action
    state, reward, terminated, truncated, info = env.step(action)
    print(f"Step {step}: Action={action}, State={state}, Reward={reward}")
    
    # Break if episode ends
    if terminated or truncated:
        print("Episode ended. Resetting...")
        state, info = env.reset()
    
    # Slow down so we can see
    time.sleep(0.02)

env.close()
print("Test complete!")
```

> ✅ Run it:
```bash
python test_env.py
```

You should see:
- A window pop up with a pole on a cart.
- The cart moving randomly.
- Terminal printing states and rewards.

🎯 Success! You’re ready for RL.

---

### 4. 🔍 Understanding `gymnasium`

`gymnasium` is the **standard interface** for RL environments.

#### Key Components:

| Component | Description |
|--------|-------------|
| `env.reset()` | Returns initial state and info |
| `env.step(action)` | Returns: `state, reward, terminated, truncated, info` |
| `env.action_space` | What actions you can take |
| `env.observation_space` | What states look like |

#### Common Environments:

| Environment | Type | Use Case |
|-----------|------|---------|
| `CartPole-v1` | Continuous state, discrete action | Balance a pole |
| `MountainCar-v0` | Continuous | Reach the flag |
| `Acrobot-v1` | Continuous | Swing up a robot arm |
| `FrozenLake-v1` | Discrete | Grid world, slippery ice |
| `LunarLander-v2` | Continuous | Land a spaceship |
| `Pendulum-v1` | Continuous action | Swing up a pendulum |

---

### 5. 🖼️ Visualizing Without `render_mode="human"`

Sometimes you don’t want a window (e.g., on a server). Use `rgb_array` to get images.

#### Example: Capture Frames

```python
import gymnasium as gym
import matplotlib.pyplot as plt

env = gym.make("CartPole-v1", render_mode="rgb_array")
env.reset()

# Take one step and grab image
_, _, _, _, _ = env.step(env.action_space.sample())
frame = env.render()  # Returns RGB array

plt.imshow(frame)
plt.title("CartPole Frame")
plt.axis("off")
plt.show()

env.close()
```

> 📈 This is how you record videos or log to TensorBoard later.

---

### 6. 🧰 Common Issues & Fixes

| Problem | Solution |
|-------|----------|
| `ModuleNotFoundError: No module named 'gymnasium'` | Run `pip install gymnasium` |
| No module named `pygame` | Run `pip install pygame` |
| `render_mode` not working | Update `gymnasium`: `pip install --upgrade gymnasium` |
| Black screen or no window | Try `render_mode="human"` and ensure GUI works |
| `env.P` not found (for FrozenLake) | Only available in **deterministic** environments |

> 🔎 Example: Access transition model in `FrozenLake`

```python
env = gym.make("FrozenLake-v1", is_slippery=False)
print("Transition model for state 0:")
# env.P[state][action] = [(prob, next_state, reward, done), ...]
for action in range(env.action_space.n):
    print(f"Action {action}: {env.P[0][action]}")
```

---

### 7. 🧪 Your First RL Loop (Template)

Let’s write a **generic RL loop** that works for any environment.

#### File: `rl_loop.py`

```python
import gymnasium as gym
import numpy as np

def run_random_agent(env_name, max_episodes=5, max_steps=200):
    # Create environment
    env = gym.make(env_name)
    total_rewards = []

    for episode in range(max_episodes):
        state, info = env.reset()
        episode_reward = 0
        print(f"\n--- Episode {episode + 1} ---")
        print(f"Initial state: {state}")

        for step in range(max_steps):
            # Choose random action
            action = env.action_space.sample()
            
            # Step the environment
            next_state, reward, terminated, truncated, info = env.step(action)
            episode_reward += reward

            # Log
            if step % 50 == 0:
                print(f"  Step {step}: Action={action}, State={next_state}, Reward={reward}")

            # Update state
            state = next_state

            # Check if episode is done
            if terminated or truncated:
                print(f"  Episode ended after {step + 1} steps. Total reward: {episode_reward}")
                break

        total_rewards.append(episode_reward)

    env.close()
    print(f"\nAverage reward over {max_episodes} episodes: {np.mean(total_rewards):.2f}")
    return total_rewards

# Run it
rewards = run_random_agent("CartPole-v1")
```

> ✅ This is your **first RL agent** — even if it's random!

---

### 8. 📊 Visualize Results

Let’s plot the rewards.

```python
import matplotlib.pyplot as plt

plt.plot(rewards, marker='o')
plt.title("Random Agent Performance")
plt.xlabel("Episode")
plt.ylabel("Total Reward")
plt.grid(True)
plt.show()
```

> 📈 You’ll see CartPole usually gets 20–50 reward before failing. Our goal? Get **200** (maximum) consistently.

---

### 9. 💡 Pro Tips for Module 0

- ✅ Always use a **virtual environment**.
- ✅ Use `gymnasium`, not `gym` (old version has bugs).
- ✅ Use `terminated` and `truncated` separately (new in Gymnasium).
- ✅ For debugging, print `state`, `action`, `reward` frequently.
- ✅ Start with `CartPole-v1` or `FrozenLake-v1` — simple and fast.

---

### 10. 🚀 What’s Next?

Now that your setup is solid, we can move to **Module 1: MDPs and Core Concepts** — where we’ll:
- Define states, actions, rewards formally
- Simulate a custom MDP
- Write a deterministic policy
- Introduce the idea of **cumulative reward**

But before we go…

👉 **Your Task (Optional):**
1. Run `rl_loop.py` with `FrozenLake-v1`
2. Change `is_slippery=True` and see what happens
3. Try `LunarLander-v2` (install `box2d` if needed: `pip install gymnasium[box2d]`)

We’re building a strong foundation — and you’re doing great. 🚀
