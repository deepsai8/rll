# Module 1: MDPs, Bellman Equations, and GridWorld
Weâ€™ll cover the **theory**, then do **two implementations**:

1. A **NumPy-only GridWorld** (manual MDP).
2. A **Gymnasium-based FrozenLake** (prebuilt MDP env).

---

# ğŸ¯ Learning Objectives (Module 1)

1. Understand **Markov Decision Processes (MDPs)** formally.
2. Learn the **Bellman Expectation & Optimality equations**.
3. Implement **Value Iteration** and **Policy Iteration**.
4. See how these algorithms solve GridWorld / FrozenLake.

---

## 1. What is an MDP?

An **MDP** is defined as a 5-tuple:

$$
\mathcal{M} = \langle S, A, P, R, \gamma \rangle
$$

* **S**: set of states
* **A**: set of actions
* **P(s' | s, a)**: transition probability (next state distribution)
* **R(s, a, s')**: reward function
* **Î³**: discount factor (0 â‰¤ Î³ < 1)

**Goal of RL:**
Find a **policy** $\pi(a|s)$ (probability distribution over actions given state) that **maximizes expected discounted return**:

$$
G_t = \sum_{k=0}^\infty \gamma^k R_{t+k+1}
$$

---

## 2. Value Functions & Bellman Equations

* **State-value function** under policy Ï€:

$$
V^\pi(s) = \mathbb{E}_\pi [ G_t \mid S_t = s ]
$$

* **Action-value function**:

$$
Q^\pi(s,a) = \mathbb{E}_\pi [ G_t \mid S_t = s, A_t = a ]
$$

* **Bellman expectation equation** for V:

$$
V^\pi(s) = \sum_a \pi(a|s) \sum_{s'} P(s'|s,a) \big[ R(s,a,s') + \gamma V^\pi(s') \big]
$$

* **Bellman optimality equation**:

$$
V^*(s) = \max_a \sum_{s'} P(s'|s,a) \big[ R(s,a,s') + \gamma V^*(s') \big]
$$

---

## 3. Value Iteration (pseudocode)

```text
Initialize V(s) arbitrarily
Repeat until convergence:
    For each state s:
        V(s) â† max_a Î£_{s'} P(s'|s,a) [ R(s,a,s') + Î³ V(s') ]
```

Then derive the optimal policy:

$$
\pi^*(s) = \arg\max_a \sum_{s'} P(s'|s,a) [ R(s,a,s') + \gamma V(s') ]
$$

---

## 4. Example 1 â€” **NumPy GridWorld**

Weâ€™ll implement a small **4x4 grid** (like Sutton & Barto).

* Agent starts anywhere, tries to reach a terminal state.
* Reward = -1 per step, 0 at terminal.

```python
import numpy as np

class GridWorld:
    def __init__(self, size=4, terminal_states=[0, 15], gamma=1.0):
        self.size = size
        self.n_states = size * size
        self.n_actions = 4  # up, right, down, left
        self.terminal_states = terminal_states
        self.gamma = gamma

    def step(self, state, action):
        if state in self.terminal_states:
            return state, 0

        row, col = divmod(state, self.size)
        if action == 0:   # up
            row = max(row - 1, 0)
        elif action == 1: # right
            col = min(col + 1, self.size - 1)
        elif action == 2: # down
            row = min(row + 1, self.size - 1)
        elif action == 3: # left
            col = max(col - 1, 0)

        next_state = row * self.size + col
        reward = -1
        return next_state, reward

def value_iteration(env, theta=1e-4):
    V = np.zeros(env.n_states)
    while True:
        delta = 0
        for s in range(env.n_states):
            if s in env.terminal_states:
                continue
            v = V[s]
            q_values = []
            for a in range(env.n_actions):
                s_next, r = env.step(s, a)
                q_values.append(r + env.gamma * V[s_next])
            V[s] = max(q_values)
            delta = max(delta, abs(v - V[s]))
        if delta < theta:
            break

    # Extract policy
    policy = np.zeros([env.n_states, env.n_actions])
    for s in range(env.n_states):
        if s in env.terminal_states:
            continue
        q_values = []
        for a in range(env.n_actions):
            s_next, r = env.step(s, a)
            q_values.append(r + env.gamma * V[s_next])
        best_a = np.argmax(q_values)
        policy[s, best_a] = 1.0
    return V.reshape(env.size, env.size), policy
```

Run it:

```python
env = GridWorld()
V, policy = value_iteration(env)
print("Optimal Value Function:")
print(V)
print("\nPolicy (arrows):")
arrows = {0: "â†‘", 1: "â†’", 2: "â†“", 3: "â†"}
policy_arrows = []
for s in range(env.n_states):
    if s in env.terminal_states:
        policy_arrows.append("T")
    else:
        policy_arrows.append(arrows[np.argmax(policy[s])])
print(np.array(policy_arrows).reshape(env.size, env.size))
```

ğŸ‘‰ This will print the optimal value grid and arrows showing the optimal policy.

---

## 5. Example 2 â€” **Gymnasium FrozenLake**

We can now repeat value iteration on **FrozenLake-v1** (discrete MDP with slippery ice).

```python
import gymnasium as gym

env = gym.make("FrozenLake-v1", is_slippery=True)
n_states = env.observation_space.n
n_actions = env.action_space.n

def value_iteration_frozenlake(env, gamma=0.99, theta=1e-8):
    V = np.zeros(n_states)
    while True:
        delta = 0
        for s in range(n_states):
            v = V[s]
            q_values = []
            for a in range(n_actions):
                q = 0
                for prob, s_next, reward, terminated in env.P[s][a]:
                    q += prob * (reward + gamma * V[s_next])
                q_values.append(q)
            V[s] = max(q_values)
            delta = max(delta, abs(v - V[s]))
        if delta < theta:
            break

    # Derive policy
    policy = np.zeros([n_states, n_actions])
    for s in range(n_states):
        q_values = []
        for a in range(n_actions):
            q = 0
            for prob, s_next, reward, terminated in env.P[s][a]:
                q += prob * (reward + gamma * V[s_next])
            q_values.append(q)
        best_a = np.argmax(q_values)
        policy[s, best_a] = 1.0
    return V, policy

V, policy = value_iteration_frozenlake(env)
print("Optimal State Values:", V)
print("Optimal Policy:", policy)
```

---

## âœ… Module 1 Milestone

By the end of Module 1, you should:

* [ ] Be able to **define an MDP** formally.
* [ ] Explain and derive the **Bellman expectation & optimality equations**.
* [ ] Implement **Value Iteration** (NumPy only).
* [ ] Solve FrozenLake with value iteration.

---

ğŸ‘‰ Next step: we can do **Policy Iteration** as well (similar but with evaluation + improvement cycles).

---

Perfect ğŸ‘ Letâ€™s extend Module 1 with **Policy Iteration**.
We already covered **Value Iteration** (backup equation with max).
Now weâ€™ll do **Policy Iteration** â†’ it alternates between:

---

# ğŸ”„ Policy Iteration Algorithm

1. **Policy Evaluation**
   Given a fixed policy $\pi$, compute its value function $V^\pi$.

   * Iteratively apply the Bellman expectation equation:

   $$
   V^\pi(s) = \sum_a \pi(a|s)\sum_{s'} P(s'|s,a)[R(s,a,s') + \gamma V^\pi(s')]
   $$

2. **Policy Improvement**
   For each state, choose the greedy action with respect to $V^\pi$:

   $$
   \pi'(s) = \arg\max_a \sum_{s'} P(s'|s,a) [R(s,a,s') + \gamma V^\pi(s')]
   $$

3. If policy stabilizes (no change), stop.

**Guarantee**: Policy iteration converges to the optimal policy $\pi^*$ in finite MDPs.

---

# ğŸ§© Example 1: NumPy GridWorld (from scratch)

```python
import numpy as np

class GridWorld:
    def __init__(self, size=4, terminal_states=[0, 15], gamma=1.0):
        self.size = size
        self.n_states = size * size
        self.n_actions = 4  # up, right, down, left
        self.terminal_states = terminal_states
        self.gamma = gamma

    def step(self, state, action):
        if state in self.terminal_states:
            return state, 0

        row, col = divmod(state, self.size)
        if action == 0:   # up
            row = max(row - 1, 0)
        elif action == 1: # right
            col = min(col + 1, self.size - 1)
        elif action == 2: # down
            row = min(row + 1, self.size - 1)
        elif action == 3: # left
            col = max(col - 1, 0)

        next_state = row * self.size + col
        reward = -1
        return next_state, reward

def policy_evaluation(policy, env, V, theta=1e-4):
    while True:
        delta = 0
        for s in range(env.n_states):
            if s in env.terminal_states:
                continue
            v = V[s]
            new_v = 0
            for a, action_prob in enumerate(policy[s]):
                s_next, r = env.step(s, a)
                new_v += action_prob * (r + env.gamma * V[s_next])
            V[s] = new_v
            delta = max(delta, abs(v - new_v))
        if delta < theta:
            break
    return V

def policy_improvement(V, env):
    policy = np.zeros([env.n_states, env.n_actions])
    for s in range(env.n_states):
        if s in env.terminal_states:
            continue
        q_values = []
        for a in range(env.n_actions):
            s_next, r = env.step(s, a)
            q_values.append(r + env.gamma * V[s_next])
        best_a = np.argmax(q_values)
        policy[s, best_a] = 1.0
    return policy

def policy_iteration(env):
    # Start with random policy (uniform)
    policy = np.ones([env.n_states, env.n_actions]) / env.n_actions
    V = np.zeros(env.n_states)

    while True:
        V = policy_evaluation(policy, env, V)
        new_policy = policy_improvement(V, env)
        if np.array_equal(new_policy, policy):
            break
        policy = new_policy
    return V.reshape(env.size, env.size), policy
```

Run it:

```python
env = GridWorld()
V, policy = policy_iteration(env)
print("Optimal Value Function:")
print(V)

arrows = {0: "â†‘", 1: "â†’", 2: "â†“", 3: "â†"}
policy_arrows = []
for s in range(env.n_states):
    if s in env.terminal_states:
        policy_arrows.append("T")
    else:
        policy_arrows.append(arrows[np.argmax(policy[s])])
print("\nOptimal Policy (arrows):")
print(np.array(policy_arrows).reshape(env.size, env.size))
```

ğŸ‘‰ This produces the **same optimal policy** as Value Iteration, but via repeated policy evaluation & improvement cycles.

---

# ğŸ§Š Example 2: Gymnasium FrozenLake (with transition probs)

```python
import gymnasium as gym
import numpy as np

env = gym.make("FrozenLake-v1", is_slippery=True)
n_states = env.observation_space.n
n_actions = env.action_space.n

def policy_evaluation_frozenlake(policy, V, env, gamma=0.99, theta=1e-8):
    while True:
        delta = 0
        for s in range(n_states):
            v = V[s]
            new_v = 0
            for a, action_prob in enumerate(policy[s]):
                for prob, s_next, reward, terminated in env.P[s][a]:
                    new_v += action_prob * prob * (reward + gamma * V[s_next])
            V[s] = new_v
            delta = max(delta, abs(v - new_v))
        if delta < theta:
            break
    return V

def policy_improvement_frozenlake(V, env, gamma=0.99):
    policy = np.zeros([n_states, n_actions])
    for s in range(n_states):
        q_values = np.zeros(n_actions)
        for a in range(n_actions):
            for prob, s_next, reward, terminated in env.P[s][a]:
                q_values[a] += prob * (reward + gamma * V[s_next])
        best_a = np.argmax(q_values)
        policy[s, best_a] = 1.0
    return policy

def policy_iteration_frozenlake(env, gamma=0.99):
    policy = np.ones([n_states, n_actions]) / n_actions
    V = np.zeros(n_states)

    while True:
        V = policy_evaluation_frozenlake(policy, V, env, gamma)
        new_policy = policy_improvement_frozenlake(V, env, gamma)
        if np.array_equal(new_policy, policy):
            break
        policy = new_policy
    return V, policy

V, policy = policy_iteration_frozenlake(env)
print("Optimal Values:", V)
print("Optimal Policy (deterministic actions):")
print(policy)
```

---

# ğŸ”‘ Key Takeaways

* **Value Iteration**: directly applies Bellman *optimality* updates.
* **Policy Iteration**: alternates between evaluating a policy and improving it.
* Both converge to **optimal policy** $\pi^*$.
* Value Iteration is often faster in practice, but Policy Iteration gives insight into policy/value interaction.

---

âœ… Now youâ€™ve completed **Module 1** fully:

* [x] Defined MDPs
* [x] Learned Bellman equations
* [x] Implemented Value Iteration
* [x] Implemented Policy Iteration
* [x] Solved GridWorld & FrozenLake

---



# structure 2

ğŸš€ **Perfect!** Letâ€™s dive into **Module 1: MDPs and Core Concepts** â€” the **mathematical foundation** of Reinforcement Learning.

Weâ€™ll go from intuition to code, step by step:
- What is an **MDP**?
- The 5 components of an MDP
- States, Actions, Rewards, Transitions, Discount
- How environments like `FrozenLake` or `CartPole` are MDPs
- Code a **custom MDP from scratch**
- Simulate agent-environment interaction

Letâ€™s begin!

---

## ğŸ”¹ Module 1: MDPs and Core Concepts  
**Markov Decision Processes â€” The Language of RL**

---

### ğŸ¯ Goal:
Understand that **every RL problem is an MDP**, and learn to think in terms of:
- States
- Actions
- Transitions
- Rewards
- Policies

---

### ğŸ§© 1. What is an MDP?

An **MDP (Markov Decision Process)** is a mathematical framework for modeling decision-making in environments where outcomes are partly random and partly under the agentâ€™s control.

Itâ€™s defined by a 5-tuple:  
\[
(S, A, P, R, \gamma)
\]

| Symbol | Meaning |
|-------|--------|
| \( S \) | Set of **States** |
| \( A \) | Set of **Actions** |
| \( P \) | **Transition function**: \( P(s' | s, a) \) |
| \( R \) | **Reward function**: \( R(s, a, s') \) or \( R(s, a) \) |
| \( \gamma \) | **Discount factor**: \( 0 \leq \gamma \leq 1 \) |

> âœ… The **Markov Property**: The next state depends only on the current state and action â€” **not on history**.

---

### ğŸŒ 2. Real-World Analogy

Imagine you're playing a video game:
- **State**: What you see on screen (e.g., player position, enemy locations)
- **Action**: Press "up", "shoot", etc.
- **Transition**: Game engine updates the world (sometimes randomly â€” e.g., enemy movement)
- **Reward**: +10 for killing enemy, -1 for getting hit
- **Discount Î³**: You care more about immediate rewards than distant ones

This is an MDP.

---

### ğŸ§± 3. Components of an MDP (with Code)

Letâ€™s build a **tiny MDP** from scratch: a **2x2 Grid World**.

#### ğŸ—ºï¸ Environment: 2x2 Grid
```
States:
0 | 1
-----
2 | 3

Goal: Reach state 3 (bottom-right)
Actions: 0=â†‘, 1=â†’, 2=â†“, 3=â†
Reward: +1 for reaching 3, 0 otherwise
Deterministic transitions
```

---

### ğŸ§ª Step 1: Define the MDP in Python

```python
# Define the MDP
class GridWorld:
    def __init__(self):
        self.states = [0, 1, 2, 3]          # 4 states
        self.actions = [0, 1, 2, 3]         # Up, Right, Down, Left
        self.goal = 3
        self.gamma = 0.9                    # Discount factor

        # Transition and reward model: P[s][a] = (s', r)
        self.P = {
            0: {  # State 0 (top-left)
                0: (0, 0),  # Up â†’ stay
                1: (1, 0),  # Right â†’ 1
                2: (2, 0),  # Down â†’ 2
                3: (0, 0),  # Left â†’ stay
            },
            1: {  # State 1 (top-right)
                0: (1, 0),
                1: (1, 0),  # Can't go right
                2: (3, 1),  # Down â†’ goal! +1 reward
                3: (0, 0),  # Left â†’ 0
            },
            2: {  # State 2 (bottom-left)
                0: (0, 0),
                1: (3, 1),  # Right â†’ goal! +1 reward
                2: (2, 0),
                3: (2, 0),
            },
            3: {  # State 3 (goal)
                0: (3, 0),
                1: (3, 0),
                2: (3, 0),
                3: (3, 0),  # Stay in goal
            }
        }

    def reset(self):
        """Start at random non-goal state"""
        self.state = np.random.choice([0, 1, 2])
        return self.state, {}

    def step(self, action):
        """Execute action"""
        next_state, reward = self.P[self.state][action]
        self.state = next_state
        terminated = (next_state == self.goal)
        truncated = False
        info = {}
        return next_state, reward, terminated, truncated, info

    def render(self):
        grid = np.array(['â¬œ', 'â¬œ', 'â¬œ', 'ğŸ†']).reshape(2,2)
        pos = {0: (0,0), 1: (0,1), 2: (1,0), 3: (1,1)}
        agent_pos = [k for k,v in pos.items() if k == self.state][0]
        grid_display = grid.copy()
        if self.state != 3:
            i, j = pos[self.state]
            grid_display[i, j] = 'ğŸ¤–'
        print("\n".join(["".join(row) for row in grid_display]))
```

---

### ğŸ§ª Step 2: Test the MDP

```python
env = GridWorld()
state, info = env.reset()
print(f"Start state: {state}")
env.render()

# Take a few steps
for step in range(5):
    action = 1  # Always go right
    next_state, reward, terminated, truncated, info = env.step(action)
    print(f"Step {step+1}: Action=Right â†’ State {next_state}, Reward={reward}")
    env.render()
    if terminated:
        print("ğŸ‰ Goal reached!")
        break
```

> âœ… Youâ€™ll see the agent move toward the goal.

---

### ğŸ” 4. Key Concepts in Action

Letâ€™s map our code to MDP theory:

| Concept | In Our Code |
|-------|------------|
| \( S \) | `[0,1,2,3]` |
| \( A \) | `[0,1,2,3]` |
| \( P(s'\|s,a) \) | `self.P[s][a] = (s', r)` |
| \( R(s,a,s') \) | Embedded in transition (reward returned) |
| \( \gamma \) | `0.9` â€” weâ€™ll use this later in value functions |

> ğŸ§  Note: In real RL, we often **donâ€™t know P and R** â€” we have to learn them.

---

### ğŸ”„ 5. The RL Loop (General Form)

This is the **core loop** of all RL algorithms:

```python
state, info = env.reset()
total_reward = 0
for step in range(max_steps):
    action = policy(state)              # Agent chooses action
    next_state, reward, terminated, truncated, info = env.step(action)
    total_reward += reward
    
    # Here's where learning happens (later)
    # update_value_function(state, action, reward, next_state)
    
    state = next_state
    if terminated or truncated:
        break
```

This loop is **universal** â€” whether you're using Q-learning, PPO, or DQN.

---

### ğŸ® 6. Compare with `gymnasium` Environments

Letâ€™s see how our custom MDP compares to `FrozenLake`.

```python
import gymnasium as gym

# Built-in FrozenLake
env_gym = gym.make("FrozenLake-v1", is_slippery=False)

print("Our GridWorld:")
print(" States:", env.states)
print(" Actions:", env.actions)
print(" Goal: state 3\n")

print("FrozenLake:")
print(" States:", env_gym.observation_space.n)
print(" Actions:", env_gym.action_space.n)
print("Slippery?", gym.make("FrozenLake-v1").unwrapped.slippery)
```

> ğŸ” Both are 4x4 or 2x2 grid MDPs â€” just different representations.

---

### ğŸ“ˆ 7. Why Discounting? (\( \gamma \))

Without discounting, infinite-horizon tasks can have **infinite returns**.

With \( \gamma < 1 \), future rewards are worth less.

\[
G_t = r_{t+1} + \gamma r_{t+2} + \gamma^2 r_{t+3} + \dots
\]

#### Example:
- Reward now: +1
- Reward in 10 steps: \( \gamma^{10} \times 1 \)
- If \( \gamma = 0.9 \), then \( 0.9^{10} \approx 0.35 \)

ğŸ‘‰ The agent prefers **shorter paths to reward**.

---

### ğŸ§  8. Markov Property â€” Why It Matters

The **Markov Property** means:
\[
P(S_{t+1} | S_t, A_t) = P(S_{t+1} | S_0, A_0, \dots, S_t, A_t)
\]

In other words: **you donâ€™t need history** â€” the current state has all necessary info.

> âœ… This allows us to define value functions like \( V(s) \) â€” they depend only on \( s \), not on how you got there.

---

### âœ… 9. Summary: MDP Checklist

An environment is an MDP if it has:
- âœ… Well-defined **states**
- âœ… Well-defined **actions**
- âœ… **Transition probabilities** (deterministic or stochastic)
- âœ… **Rewards** tied to transitions
- âœ… **Discount factor** (optional for episodic tasks)

> ğŸ¯ Almost all RL problems (games, robots, finance) can be modeled as MDPs.

---

### ğŸš€ 10. Your Task (Optional but Recommended)

1. **Modify the GridWorld**:
   - Add stochasticity (e.g., "Right" only works 80% of the time)
   - Add a "lava" state that gives -1 reward

2. **Run a random agent** on it and plot total rewards over 100 episodes.

3. **Compare** with `FrozenLake-v1` â€” how are they similar?

---

### ğŸ”œ Next: Module 2 â€” Policies, Value Functions & Bellman Equations

Now that we understand **MDPs**, weâ€™ll learn how agents **evaluate and improve behavior** using:
- **Policies** \( \pi(a|s) \)
- **Value functions** \( V(s) \), \( Q(s,a) \)
- **Bellman equations**
- **Policy evaluation via simulation**

Would you like to:
- âœ… Continue now?
- ğŸ’¬ Discuss anything from Module 1?
- ğŸ› ï¸ Fix a bug or extend the GridWorld?

Let me know â€” weâ€™re building strong foundations! ğŸ—ï¸