# Module 1: MDPs, Bellman Equations, and GridWorld
We’ll cover the **theory**, then do **two implementations**:

1. A **NumPy-only GridWorld** (manual MDP).
2. A **Gymnasium-based FrozenLake** (prebuilt MDP env).

---

# 🎯 Learning Objectives (Module 1)

1. Understand **Markov Decision Processes (MDPs)** formally.
2. Learn the **Bellman Expectation & Optimality equations**.
3. Implement **Value Iteration** and **Policy Iteration**.
4. See how these algorithms solve GridWorld / FrozenLake.

---

## 1. What is an MDP?

An **MDP** is defined as a 5-tuple:

$$
\mathcal{M} = \langle S, A, P, R, \gamma \rangle
$$

* **S**: set of states
* **A**: set of actions
* **P(s' | s, a)**: transition probability (next state distribution)
* **R(s, a, s')**: reward function
* **γ**: discount factor (0 ≤ γ < 1)

**Goal of RL:**
Find a **policy** $\pi(a|s)$ (probability distribution over actions given state) that **maximizes expected discounted return**:

$$
G_t = \sum_{k=0}^\infty \gamma^k R_{t+k+1}
$$

---

## 2. Value Functions & Bellman Equations

* **State-value function** under policy π:

$$
V^\pi(s) = \mathbb{E}_\pi [ G_t \mid S_t = s ]
$$

* **Action-value function**:

$$
Q^\pi(s,a) = \mathbb{E}_\pi [ G_t \mid S_t = s, A_t = a ]
$$

* **Bellman expectation equation** for V:

$$
V^\pi(s) = \sum_a \pi(a|s) \sum_{s'} P(s'|s,a) \big[ R(s,a,s') + \gamma V^\pi(s') \big]
$$

* **Bellman optimality equation**:

$$
V^*(s) = \max_a \sum_{s'} P(s'|s,a) \big[ R(s,a,s') + \gamma V^*(s') \big]
$$

---

## 3. Value Iteration (pseudocode)

```text
Initialize V(s) arbitrarily
Repeat until convergence:
    For each state s:
        V(s) ← max_a Σ_{s'} P(s'|s,a) [ R(s,a,s') + γ V(s') ]
```

Then derive the optimal policy:

$$
\pi^*(s) = \arg\max_a \sum_{s'} P(s'|s,a) [ R(s,a,s') + \gamma V(s') ]
$$

---

## 4. Example 1 — **NumPy GridWorld**

We’ll implement a small **4x4 grid** (like Sutton & Barto).

* Agent starts anywhere, tries to reach a terminal state.
* Reward = -1 per step, 0 at terminal.

```python
import numpy as np

class GridWorld:
    def __init__(self, size=4, terminal_states=[0, 15], gamma=1.0):
        self.size = size
        self.n_states = size * size
        self.n_actions = 4  # up, right, down, left
        self.terminal_states = terminal_states
        self.gamma = gamma

    def step(self, state, action):
        if state in self.terminal_states:
            return state, 0

        row, col = divmod(state, self.size)
        if action == 0:   # up
            row = max(row - 1, 0)
        elif action == 1: # right
            col = min(col + 1, self.size - 1)
        elif action == 2: # down
            row = min(row + 1, self.size - 1)
        elif action == 3: # left
            col = max(col - 1, 0)

        next_state = row * self.size + col
        reward = -1
        return next_state, reward

def value_iteration(env, theta=1e-4):
    V = np.zeros(env.n_states)
    while True:
        delta = 0
        for s in range(env.n_states):
            if s in env.terminal_states:
                continue
            v = V[s]
            q_values = []
            for a in range(env.n_actions):
                s_next, r = env.step(s, a)
                q_values.append(r + env.gamma * V[s_next])
            V[s] = max(q_values)
            delta = max(delta, abs(v - V[s]))
        if delta < theta:
            break

    # Extract policy
    policy = np.zeros([env.n_states, env.n_actions])
    for s in range(env.n_states):
        if s in env.terminal_states:
            continue
        q_values = []
        for a in range(env.n_actions):
            s_next, r = env.step(s, a)
            q_values.append(r + env.gamma * V[s_next])
        best_a = np.argmax(q_values)
        policy[s, best_a] = 1.0
    return V.reshape(env.size, env.size), policy
```

Run it:

```python
env = GridWorld()
V, policy = value_iteration(env)
print("Optimal Value Function:")
print(V)
print("\nPolicy (arrows):")
arrows = {0: "↑", 1: "→", 2: "↓", 3: "←"}
policy_arrows = []
for s in range(env.n_states):
    if s in env.terminal_states:
        policy_arrows.append("T")
    else:
        policy_arrows.append(arrows[np.argmax(policy[s])])
print(np.array(policy_arrows).reshape(env.size, env.size))
```

👉 This will print the optimal value grid and arrows showing the optimal policy.

---

## 5. Example 2 — **Gymnasium FrozenLake**

We can now repeat value iteration on **FrozenLake-v1** (discrete MDP with slippery ice).

```python
import gymnasium as gym

env = gym.make("FrozenLake-v1", is_slippery=True)
n_states = env.observation_space.n
n_actions = env.action_space.n

def value_iteration_frozenlake(env, gamma=0.99, theta=1e-8):
    V = np.zeros(n_states)
    while True:
        delta = 0
        for s in range(n_states):
            v = V[s]
            q_values = []
            for a in range(n_actions):
                q = 0
                for prob, s_next, reward, terminated in env.P[s][a]:
                    q += prob * (reward + gamma * V[s_next])
                q_values.append(q)
            V[s] = max(q_values)
            delta = max(delta, abs(v - V[s]))
        if delta < theta:
            break

    # Derive policy
    policy = np.zeros([n_states, n_actions])
    for s in range(n_states):
        q_values = []
        for a in range(n_actions):
            q = 0
            for prob, s_next, reward, terminated in env.P[s][a]:
                q += prob * (reward + gamma * V[s_next])
            q_values.append(q)
        best_a = np.argmax(q_values)
        policy[s, best_a] = 1.0
    return V, policy

V, policy = value_iteration_frozenlake(env)
print("Optimal State Values:", V)
print("Optimal Policy:", policy)
```

---

## ✅ Module 1 Milestone

By the end of Module 1, you should:

* [ ] Be able to **define an MDP** formally.
* [ ] Explain and derive the **Bellman expectation & optimality equations**.
* [ ] Implement **Value Iteration** (NumPy only).
* [ ] Solve FrozenLake with value iteration.

---

👉 Next step: we can do **Policy Iteration** as well (similar but with evaluation + improvement cycles).

---

Perfect 👍 Let’s extend Module 1 with **Policy Iteration**.
We already covered **Value Iteration** (backup equation with max).
Now we’ll do **Policy Iteration** → it alternates between:

---

# 🔄 Policy Iteration Algorithm

1. **Policy Evaluation**
   Given a fixed policy $\pi$, compute its value function $V^\pi$.

   * Iteratively apply the Bellman expectation equation:

   $$
   V^\pi(s) = \sum_a \pi(a|s)\sum_{s'} P(s'|s,a)[R(s,a,s') + \gamma V^\pi(s')]
   $$

2. **Policy Improvement**
   For each state, choose the greedy action with respect to $V^\pi$:

   $$
   \pi'(s) = \arg\max_a \sum_{s'} P(s'|s,a) [R(s,a,s') + \gamma V^\pi(s')]
   $$

3. If policy stabilizes (no change), stop.

**Guarantee**: Policy iteration converges to the optimal policy $\pi^*$ in finite MDPs.

---

# 🧩 Example 1: NumPy GridWorld (from scratch)

```python
import numpy as np

class GridWorld:
    def __init__(self, size=4, terminal_states=[0, 15], gamma=1.0):
        self.size = size
        self.n_states = size * size
        self.n_actions = 4  # up, right, down, left
        self.terminal_states = terminal_states
        self.gamma = gamma

    def step(self, state, action):
        if state in self.terminal_states:
            return state, 0

        row, col = divmod(state, self.size)
        if action == 0:   # up
            row = max(row - 1, 0)
        elif action == 1: # right
            col = min(col + 1, self.size - 1)
        elif action == 2: # down
            row = min(row + 1, self.size - 1)
        elif action == 3: # left
            col = max(col - 1, 0)

        next_state = row * self.size + col
        reward = -1
        return next_state, reward

def policy_evaluation(policy, env, V, theta=1e-4):
    while True:
        delta = 0
        for s in range(env.n_states):
            if s in env.terminal_states:
                continue
            v = V[s]
            new_v = 0
            for a, action_prob in enumerate(policy[s]):
                s_next, r = env.step(s, a)
                new_v += action_prob * (r + env.gamma * V[s_next])
            V[s] = new_v
            delta = max(delta, abs(v - new_v))
        if delta < theta:
            break
    return V

def policy_improvement(V, env):
    policy = np.zeros([env.n_states, env.n_actions])
    for s in range(env.n_states):
        if s in env.terminal_states:
            continue
        q_values = []
        for a in range(env.n_actions):
            s_next, r = env.step(s, a)
            q_values.append(r + env.gamma * V[s_next])
        best_a = np.argmax(q_values)
        policy[s, best_a] = 1.0
    return policy

def policy_iteration(env):
    # Start with random policy (uniform)
    policy = np.ones([env.n_states, env.n_actions]) / env.n_actions
    V = np.zeros(env.n_states)

    while True:
        V = policy_evaluation(policy, env, V)
        new_policy = policy_improvement(V, env)
        if np.array_equal(new_policy, policy):
            break
        policy = new_policy
    return V.reshape(env.size, env.size), policy
```

Run it:

```python
env = GridWorld()
V, policy = policy_iteration(env)
print("Optimal Value Function:")
print(V)

arrows = {0: "↑", 1: "→", 2: "↓", 3: "←"}
policy_arrows = []
for s in range(env.n_states):
    if s in env.terminal_states:
        policy_arrows.append("T")
    else:
        policy_arrows.append(arrows[np.argmax(policy[s])])
print("\nOptimal Policy (arrows):")
print(np.array(policy_arrows).reshape(env.size, env.size))
```

👉 This produces the **same optimal policy** as Value Iteration, but via repeated policy evaluation & improvement cycles.

---

# 🧊 Example 2: Gymnasium FrozenLake (with transition probs)

```python
import gymnasium as gym
import numpy as np

env = gym.make("FrozenLake-v1", is_slippery=True)
n_states = env.observation_space.n
n_actions = env.action_space.n

def policy_evaluation_frozenlake(policy, V, env, gamma=0.99, theta=1e-8):
    while True:
        delta = 0
        for s in range(n_states):
            v = V[s]
            new_v = 0
            for a, action_prob in enumerate(policy[s]):
                for prob, s_next, reward, terminated in env.P[s][a]:
                    new_v += action_prob * prob * (reward + gamma * V[s_next])
            V[s] = new_v
            delta = max(delta, abs(v - new_v))
        if delta < theta:
            break
    return V

def policy_improvement_frozenlake(V, env, gamma=0.99):
    policy = np.zeros([n_states, n_actions])
    for s in range(n_states):
        q_values = np.zeros(n_actions)
        for a in range(n_actions):
            for prob, s_next, reward, terminated in env.P[s][a]:
                q_values[a] += prob * (reward + gamma * V[s_next])
        best_a = np.argmax(q_values)
        policy[s, best_a] = 1.0
    return policy

def policy_iteration_frozenlake(env, gamma=0.99):
    policy = np.ones([n_states, n_actions]) / n_actions
    V = np.zeros(n_states)

    while True:
        V = policy_evaluation_frozenlake(policy, V, env, gamma)
        new_policy = policy_improvement_frozenlake(V, env, gamma)
        if np.array_equal(new_policy, policy):
            break
        policy = new_policy
    return V, policy

V, policy = policy_iteration_frozenlake(env)
print("Optimal Values:", V)
print("Optimal Policy (deterministic actions):")
print(policy)
```

---

# 🔑 Key Takeaways

* **Value Iteration**: directly applies Bellman *optimality* updates.
* **Policy Iteration**: alternates between evaluating a policy and improving it.
* Both converge to **optimal policy** $\pi^*$.
* Value Iteration is often faster in practice, but Policy Iteration gives insight into policy/value interaction.

---

✅ Now you’ve completed **Module 1** fully:

* [x] Defined MDPs
* [x] Learned Bellman equations
* [x] Implemented Value Iteration
* [x] Implemented Policy Iteration
* [x] Solved GridWorld & FrozenLake

---



# structure 2

🚀 **Perfect!** Let’s dive into **Module 1: MDPs and Core Concepts** — the **mathematical foundation** of Reinforcement Learning.

We’ll go from intuition to code, step by step:
- What is an **MDP**?
- The 5 components of an MDP
- States, Actions, Rewards, Transitions, Discount
- How environments like `FrozenLake` or `CartPole` are MDPs
- Code a **custom MDP from scratch**
- Simulate agent-environment interaction

Let’s begin!

---

## 🔹 Module 1: MDPs and Core Concepts  
**Markov Decision Processes — The Language of RL**

---

### 🎯 Goal:
Understand that **every RL problem is an MDP**, and learn to think in terms of:
- States
- Actions
- Transitions
- Rewards
- Policies

---

### 🧩 1. What is an MDP?

An **MDP (Markov Decision Process)** is a mathematical framework for modeling decision-making in environments where outcomes are partly random and partly under the agent’s control.

It’s defined by a 5-tuple:  
\[
(S, A, P, R, \gamma)
\]

| Symbol | Meaning |
|-------|--------|
| \( S \) | Set of **States** |
| \( A \) | Set of **Actions** |
| \( P \) | **Transition function**: \( P(s' | s, a) \) |
| \( R \) | **Reward function**: \( R(s, a, s') \) or \( R(s, a) \) |
| \( \gamma \) | **Discount factor**: \( 0 \leq \gamma \leq 1 \) |

> ✅ The **Markov Property**: The next state depends only on the current state and action — **not on history**.

---

### 🌍 2. Real-World Analogy

Imagine you're playing a video game:
- **State**: What you see on screen (e.g., player position, enemy locations)
- **Action**: Press "up", "shoot", etc.
- **Transition**: Game engine updates the world (sometimes randomly — e.g., enemy movement)
- **Reward**: +10 for killing enemy, -1 for getting hit
- **Discount γ**: You care more about immediate rewards than distant ones

This is an MDP.

---

### 🧱 3. Components of an MDP (with Code)

Let’s build a **tiny MDP** from scratch: a **2x2 Grid World**.

#### 🗺️ Environment: 2x2 Grid
```
States:
0 | 1
-----
2 | 3

Goal: Reach state 3 (bottom-right)
Actions: 0=↑, 1=→, 2=↓, 3=←
Reward: +1 for reaching 3, 0 otherwise
Deterministic transitions
```

---

### 🧪 Step 1: Define the MDP in Python

```python
# Define the MDP
class GridWorld:
    def __init__(self):
        self.states = [0, 1, 2, 3]          # 4 states
        self.actions = [0, 1, 2, 3]         # Up, Right, Down, Left
        self.goal = 3
        self.gamma = 0.9                    # Discount factor

        # Transition and reward model: P[s][a] = (s', r)
        self.P = {
            0: {  # State 0 (top-left)
                0: (0, 0),  # Up → stay
                1: (1, 0),  # Right → 1
                2: (2, 0),  # Down → 2
                3: (0, 0),  # Left → stay
            },
            1: {  # State 1 (top-right)
                0: (1, 0),
                1: (1, 0),  # Can't go right
                2: (3, 1),  # Down → goal! +1 reward
                3: (0, 0),  # Left → 0
            },
            2: {  # State 2 (bottom-left)
                0: (0, 0),
                1: (3, 1),  # Right → goal! +1 reward
                2: (2, 0),
                3: (2, 0),
            },
            3: {  # State 3 (goal)
                0: (3, 0),
                1: (3, 0),
                2: (3, 0),
                3: (3, 0),  # Stay in goal
            }
        }

    def reset(self):
        """Start at random non-goal state"""
        self.state = np.random.choice([0, 1, 2])
        return self.state, {}

    def step(self, action):
        """Execute action"""
        next_state, reward = self.P[self.state][action]
        self.state = next_state
        terminated = (next_state == self.goal)
        truncated = False
        info = {}
        return next_state, reward, terminated, truncated, info

    def render(self):
        grid = np.array(['⬜', '⬜', '⬜', '🏆']).reshape(2,2)
        pos = {0: (0,0), 1: (0,1), 2: (1,0), 3: (1,1)}
        agent_pos = [k for k,v in pos.items() if k == self.state][0]
        grid_display = grid.copy()
        if self.state != 3:
            i, j = pos[self.state]
            grid_display[i, j] = '🤖'
        print("\n".join(["".join(row) for row in grid_display]))
```

---

### 🧪 Step 2: Test the MDP

```python
env = GridWorld()
state, info = env.reset()
print(f"Start state: {state}")
env.render()

# Take a few steps
for step in range(5):
    action = 1  # Always go right
    next_state, reward, terminated, truncated, info = env.step(action)
    print(f"Step {step+1}: Action=Right → State {next_state}, Reward={reward}")
    env.render()
    if terminated:
        print("🎉 Goal reached!")
        break
```

> ✅ You’ll see the agent move toward the goal.

---

### 🔍 4. Key Concepts in Action

Let’s map our code to MDP theory:

| Concept | In Our Code |
|-------|------------|
| \( S \) | `[0,1,2,3]` |
| \( A \) | `[0,1,2,3]` |
| \( P(s'\|s,a) \) | `self.P[s][a] = (s', r)` |
| \( R(s,a,s') \) | Embedded in transition (reward returned) |
| \( \gamma \) | `0.9` — we’ll use this later in value functions |

> 🧠 Note: In real RL, we often **don’t know P and R** — we have to learn them.

---

### 🔄 5. The RL Loop (General Form)

This is the **core loop** of all RL algorithms:

```python
state, info = env.reset()
total_reward = 0
for step in range(max_steps):
    action = policy(state)              # Agent chooses action
    next_state, reward, terminated, truncated, info = env.step(action)
    total_reward += reward
    
    # Here's where learning happens (later)
    # update_value_function(state, action, reward, next_state)
    
    state = next_state
    if terminated or truncated:
        break
```

This loop is **universal** — whether you're using Q-learning, PPO, or DQN.

---

### 🎮 6. Compare with `gymnasium` Environments

Let’s see how our custom MDP compares to `FrozenLake`.

```python
import gymnasium as gym

# Built-in FrozenLake
env_gym = gym.make("FrozenLake-v1", is_slippery=False)

print("Our GridWorld:")
print(" States:", env.states)
print(" Actions:", env.actions)
print(" Goal: state 3\n")

print("FrozenLake:")
print(" States:", env_gym.observation_space.n)
print(" Actions:", env_gym.action_space.n)
print("Slippery?", gym.make("FrozenLake-v1").unwrapped.slippery)
```

> 🔁 Both are 4x4 or 2x2 grid MDPs — just different representations.

---

### 📈 7. Why Discounting? (\( \gamma \))

Without discounting, infinite-horizon tasks can have **infinite returns**.

With \( \gamma < 1 \), future rewards are worth less.

\[
G_t = r_{t+1} + \gamma r_{t+2} + \gamma^2 r_{t+3} + \dots
\]

#### Example:
- Reward now: +1
- Reward in 10 steps: \( \gamma^{10} \times 1 \)
- If \( \gamma = 0.9 \), then \( 0.9^{10} \approx 0.35 \)

👉 The agent prefers **shorter paths to reward**.

---

### 🧠 8. Markov Property — Why It Matters

The **Markov Property** means:
\[
P(S_{t+1} | S_t, A_t) = P(S_{t+1} | S_0, A_0, \dots, S_t, A_t)
\]

In other words: **you don’t need history** — the current state has all necessary info.

> ✅ This allows us to define value functions like \( V(s) \) — they depend only on \( s \), not on how you got there.

---

### ✅ 9. Summary: MDP Checklist

An environment is an MDP if it has:
- ✅ Well-defined **states**
- ✅ Well-defined **actions**
- ✅ **Transition probabilities** (deterministic or stochastic)
- ✅ **Rewards** tied to transitions
- ✅ **Discount factor** (optional for episodic tasks)

> 🎯 Almost all RL problems (games, robots, finance) can be modeled as MDPs.

---

### 🚀 10. Your Task (Optional but Recommended)

1. **Modify the GridWorld**:
   - Add stochasticity (e.g., "Right" only works 80% of the time)
   - Add a "lava" state that gives -1 reward

2. **Run a random agent** on it and plot total rewards over 100 episodes.

3. **Compare** with `FrozenLake-v1` — how are they similar?

---

### 🔜 Next: Module 2 — Policies, Value Functions & Bellman Equations

Now that we understand **MDPs**, we’ll learn how agents **evaluate and improve behavior** using:
- **Policies** \( \pi(a|s) \)
- **Value functions** \( V(s) \), \( Q(s,a) \)
- **Bellman equations**
- **Policy evaluation via simulation**

Would you like to:
- ✅ Continue now?
- 💬 Discuss anything from Module 1?
- 🛠️ Fix a bug or extend the GridWorld?

Let me know — we’re building strong foundations! 🏗️